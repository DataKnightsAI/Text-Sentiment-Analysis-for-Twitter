{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    " \n",
    "Hyperparameter tuning is the adjustment of various pre-execution parameters passed to our Machine Learning models that affect their training/execution. Here we use two automated methods of choosing from a wide set of specified parameters - Grid Search (which exhaustively tries all combinations of specified parameters) and Random Search (which tries randomly sampled combinations of parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Grid Search (exhaustive) hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed: 27.6min finished\n",
      "C:\\ProgramData\\Anaconda3\\envs\\project10\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  LogisticRegression\n",
      "Best Score:  0.883580552276247\n",
      "Best Params:  {'C': 1, 'class_weight': {1: 0.4, 0: 0.6}, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# ^ disables this cell in jupyter notebook\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "       \n",
    "    #----Logistic Regression Hyperparameter Tuning----\n",
    "    \n",
    "    if model_name == 'LogisticRegression':    \n",
    "        \n",
    "        penalty = ['l1', 'l2']\n",
    "        C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "        class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "        solver = ['liblinear', 'saga']\n",
    "\n",
    "        param_grid = dict(penalty=penalty,\n",
    "                          C=C,\n",
    "                          class_weight=class_weight,\n",
    "                          solver=solver)\n",
    "\n",
    "        grid = GridSearchCV(estimator=model,\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1)\n",
    "        grid_result = grid.fit(X_train, Y_train)\n",
    "        print('Model Name: ', model_name)\n",
    "        print('Best Score: ', grid_result.best_score_)\n",
    "        print('Best Params: ', grid_result.best_params_)\n",
    "        \n",
    "        #----Gradient Boosting Hyperparameter Tuning----\n",
    "\n",
    "    if model_name == 'GradientBoostingClassifier':    \n",
    "\n",
    "        learning_rate = [0.15,0.1,0.05,0.01,0.005,0.001]\n",
    "        n_estimators = [100,250,500,750,1000,1250,1500,1750]\n",
    "        max_depth = [2,3,4,5,6,7]       \n",
    "\n",
    "        param_grid = dict(learning_rate=learning_rate,\n",
    "                          n_estimators=n_estimators,\n",
    "                          max_depth=max_depth,)\n",
    "\n",
    "        grid = GridSearchCV(estimator=model,\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1)\n",
    "        grid_result = grid.fit(X_train, Y_train)\n",
    "        print('Model Name: ', model_name)\n",
    "        print('Best Score: ', grid_result.best_score_)\n",
    "        print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Random Search hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1.          ,     2.7825594022,     7.7426368268,\n",
       "          21.5443469003,    59.9484250319,   166.81005372  ,\n",
       "         464.1588833613,  1291.5496650149,  3593.8136638046,\n",
       "       10000.          ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# ^ disables this cell in jupyter notebook\n",
    "\n",
    "#exploration of specifying a range of values using numpy's logspace function. \n",
    "#Used to specify range of C values in logistic regression.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=10)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "np.logspace(0, 4, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=14, min_samples_split=140,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=0,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 29.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  GradientBoostingClassifier\n",
      "Best Score:  0.864653979518611\n",
      "Best Params:  {'subsample': 0.9, 'n_estimators': 10, 'max_features': 'sqrt', 'max_depth': 8, 'loss': 'deviance', 'learning_rate': 0.15000000000000002, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# ^ disables this cell in jupyter notebook\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    print(model)\n",
    "    if model_name == 'LogisticRegression':    \n",
    "        \n",
    "        penalty = ['l1', 'l2']\n",
    "        C = np.logspace(0, 4, num=10)\n",
    "        class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "        solver = ['liblinear', 'saga']\n",
    "        \n",
    "        param_distributions = dict(penalty=penalty,\n",
    "                          C=C,\n",
    "                          class_weight=class_weight,\n",
    "                          solver=solver)\n",
    "\n",
    "        random = RandomizedSearchCV(estimator=model,\n",
    "                                    param_distributions=param_distributions,\n",
    "                                    scoring='roc_auc',\n",
    "                                    verbose=1, n_jobs=-1,\n",
    "                                    n_iter=100)\n",
    "        random_result = random.fit(X_train, Y_train)\n",
    "\n",
    "        print('Model Name: ', model_name)\n",
    "        print('Best Score: ', random_result.best_score_)\n",
    "        print('Best Params: ', random_result.best_params_)\n",
    "        \n",
    "    if model_name == 'GradientBoostingClassifier':    \n",
    "        \n",
    "        loss=[\"deviance\"]\n",
    "        learning_rate=np.linspace(0.05, 0.2, num=4)       \n",
    "        max_depth=[3,5,8]\n",
    "        max_features=[\"log2\",\"sqrt\"]\n",
    "        criterion=[\"friedman_mse\",  \"mae\"]\n",
    "        subsample=[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "        n_estimators=[10]   \n",
    "        \n",
    "        param_distributions = dict(loss=loss,\n",
    "                          learning_rate=learning_rate,\n",
    "                        #  min_samples_split=min_samples_split,\n",
    "                        #  min_samples_leaf=min_samples_leaf,\n",
    "                          max_depth=max_depth,\n",
    "                          max_features=max_features,\n",
    "                          criterion=criterion,\n",
    "                          subsample=subsample,\n",
    "                          n_estimators=n_estimators)\n",
    "\n",
    "        random = RandomizedSearchCV(estimator=model,\n",
    "                                    param_distributions=param_distributions,\n",
    "                                    cv=3,\n",
    "                                    scoring='roc_auc',\n",
    "                                    verbose=1, \n",
    "                                    n_jobs=-1,\n",
    "                                    n_iter=100)\n",
    "        random_result = random.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Model Name: ', model_name)\n",
    "        print('Best Score: ', random_result.best_score_)\n",
    "        print('Best Params: ', random_result.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
