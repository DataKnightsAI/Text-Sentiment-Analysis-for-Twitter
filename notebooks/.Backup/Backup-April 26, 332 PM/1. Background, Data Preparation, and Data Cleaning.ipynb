{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Background, Data Preparation, and Data Cleaning \n",
    "<br>Group: Group 11 - Alex Fung, Patrick Osborne\n",
    "<br>Dataset: Twitter US Airline Sentiment\n",
    "<br>Dataset link: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "For our course project we have chosen to conduct a sentiment analysis on a data set containing approximately 14.5 thousand tweets pertaining to 6 major US airlines. Going into this project we knew that we were interested in choosing a data set and problem that closely aligned to solving a topical business problem. We also wanted to pick a data set that would offer a certain degree of challenge and learning opportunities. \n",
    "\n",
    "Our chosen data set aligns well with these goals for several reasons. Firstly, in the dozen or so years that Twitter has existed it has contributed to a significant shift in the way that companies interact with their customers, primarily from a customer service point of view, but additionally in terms of PR, marketing and even logistics. One tweet from the right (or wrong) person can set off a landslide of responses that can quickly become out of control. This is a particular concern for the US Airline industry. Over the past several years there have been several high-profile incidents causing negative public sentiment towards US airlines. We believe that a robust sentiment analysis model – specifically focused on identifying these negative sentiments before they become a larger problem - could help airlines better manage their PR crisis response and customer service. A sentiment analysis model tuned to identify negative tweets with a high degree of accuracy could help airlines analyse and learn from past Twitter trends and to rapidly identify new ones as they are occurring.\n",
    "\n",
    "Secondly, from the perspective of data science students a Twitter data offers an interesting challenge in terms of cleaning, interpretation and prediction. As Twitter caps each message at a short character limit, complete English is rarely used. The data set is full of abbreviations, slang, emojis and Twitter functions such as hashtags, mentions and re-tweets. Cleaning these out while retaining the information in the original tweet will be important to developing an effective model. As with many customer service data sets, this one is also likely to be unbalanced towards the negative sentiment side. Though this aligns well with our goal to primarily identify negative sentiment tweets, this will be important to consider as we clean the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The dataset downloaded manually from Kaggle contained a CSV and a Sqlite database file. Both files contain the same dataset, albeit in a different format. We chose to load the data using the CSV file because the dataset was not large enough to cause issues loading issues with the Pandas library. Also, the dataset had to be loaded with `UTF-8` encoding because we wanted to retain the information from emojis, which would be lost if we used another encoding, such as `CP-1252` We checked for nulls in `airline_sentiment`, `text`, and `negativereason` columns, although the dataset provided had no issues with nulls.\n",
    "\n",
    "Below are links to the following Data Preparation Steps:\n",
    "1. [Load the data](#Load-the-data)\n",
    "\n",
    "2. [Check for nulls](#Check-for-nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that we read the Tweets.csv with 'utf-8' encoding, so that we can extract the emojis properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = pd.read_csv(\"../data/Tweets.csv\", encoding='utf-8')\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataframe shows 14640 rows/observations, and 15 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no nulls amongst the columns airline_sentiment and text, which is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['airline_sentiment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 'negativereason' has a few missing nulls, but they are null only if 'airline_sentiment' is positive or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5462"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['negativereason'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['airline_sentiment'].loc[df_tweets['negativereason'].isnull()].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "Twitter data is notoriously \"unclean\" compared to the data of other NLP applications, such as newspaper articles, reviews, etc. As mentioned in the Background, Twitter's strict and small character limit means users must figure out ways of shortening their tweets into concise sentences. In practice, this means other forms of unorthodox written communication, such as Internet slang abbreviations, emojis, emoticons, hashtags, retweets, mentions, are employed regularly by users. \n",
    "\n",
    "For some of these methods of communication, such as mentions and retweets, they provide zero or negative value to our sentiment analysis; as a result, it is in our best interest to remove such text. Mentions are used by users to direct their messages to handles of other other users, such as airline companies in our case. In this dataset, mentions almost always contained the handles of airline companies, which is not very useful if most of the tweets, regardless of positive, neutral, or negative sentiment, contain those mentions. Similarly, retweets were often the tweets of the PR Twitter accounts/handles of airline companies. Because the retweets do not reflect the true sentiment of the user's specific tweet, and the retweets themselves are often of neutral or positive sentiment (never negative, because the tweet would be deleted and the person in charge of the PR Twitter account/handle would be in serious trouble), it is logical to remove retweets from the dataset since retweets will only prove to be \"noisy\" to our sentiment analysis model. \n",
    "\n",
    "On the other hand, other methods of communication such as emojis, emoticons, and hashtags could provide positive value to our sentiment analysis, since in theory visual/text elements such as emojis, and emoticons express a variety of emotions in a visual, or pseudo-visual image, which are possibly interlinked with the sentiment of the text. Hashtags could also be useful certain hashtags are associated with topics of positive, or negative sentimentality; for example, hashtags such as `#IceBucketChallenge` are linked with positive sentiments as the hashtag was used to encourage other people to perform acts of charity in a fun, positive manner.\n",
    "\n",
    "Lastly, the text of the users will have to be cleaned to ensure the text can be properly tokenized by the Spacy model. Such actions of cleaning the text include removing HTML encoding and HTTP links, converting the text to lower case, translating Internet slang abbreviations to their full English expressions, removing stop words and numbers and punctuation, and performing the lemmatization of the remaining text.\n",
    "\n",
    "Below are links to the following Data Preparation Steps:\n",
    "1. [Remove not useful columns](#1.-Remove-not-useful-columns)\n",
    "\n",
    "2. [Create new text_cleaned column](#2.-Create-new-text_cleaned-column)\n",
    "\n",
    "3. [Remove HTML encoding](#3.-Remove-HTML-encoding)\n",
    "\n",
    "4. [Remove retweets](#4.-Remove-retweets)\n",
    "\n",
    "5. [Remove mentions](#5.-Remove-mentions)\n",
    "\n",
    "6. [Remove HTTP links](#6.-Remove-HTTP-links)\n",
    "\n",
    "7. [Extract Emojis and Emoticons](#7.-Extract-Emojis-and-Emoticons)\n",
    "\n",
    "8. [Extract and Remove Hashtags](#8.-Extract-and-Remove-Hashtags)\n",
    "\n",
    "9. [Convert text to Lowercase](#9.-Convert-text-to-Lowercase)\n",
    "\n",
    "10. [Internet Slang abbreviations](#-10.-Internet-Slang-abbreviations)\n",
    "\n",
    "11. [Removing StopWords and Punctuation and numbers, Lemmatization, and Tokenization](#11.-Removing-StopWords-and-Punctuation-and-numbers,-Lemmatization,-and-Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove not useful columns\n",
    "Looking at the dataframe, we can see some columns will likely not be useful for our purposes of sentiment analysis, such timezones, number of retweets, etc. Therefore, we select the columns that we want, or think will be useful for our data exploration and analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "                                                                                                                             text  \n",
       "0                                                                                             @VirginAmerica What @dhepburn said.  \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!  \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets[\n",
    "    ['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'text']\n",
    "]\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Create new text_cleaned column\n",
    "We create a column 'text_cleaned' that will contain the cleaned up version of 'text' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text_cleaned'] = df_tweets['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove HTML encoding\n",
    "The text has not been cleaned, as there is some HTML encoding left in the text, such as `&amp;`. We will use BeautifulSoup and `lxml` package to remove the HTML encoding from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disappointed,UNITED did NOT feed small CHILDREN on a 5 & half hour flight\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Example of what BeautifulSoup with lxml package does \n",
    "#you may need to install lxml by 'pip install lxml' for this to work, then restart kernel\n",
    "example1 = BeautifulSoup('Disappointed,UNITED did NOT feed small CHILDREN on a 5 &amp; half hour flight', 'lxml')\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove HTML Encoding from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                           @VirginAmerica What @dhepburn said.\n",
       "1                                                                                      @VirginAmerica plus you've added commercials to the experience... tacky.\n",
       "2                                                                                       @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
       "3                                    @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\n",
       "4                                                                                                       @VirginAmerica and it's a really big bad thing about it\n",
       "                                                                                  ...                                                                          \n",
       "14635                                                                                           @AmericanAir thank you we got on a different flight to Chicago.\n",
       "14636    @AmericanAir leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc\n",
       "14637                                                                                              @AmericanAir Please bring American Airlines to #BlackBerry10\n",
       "14638                   @AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\n",
       "14639                @AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?\n",
       "Name: text_cleaned, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text_cleaned'] = df_tweets['text_cleaned'].apply(lambda text: BeautifulSoup(text, 'lxml').get_text())\n",
    "df_tweets['text_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove retweets\n",
    "Retweets are denoted in 'text' column as 'RT @another_user another_user's tweet'. We should remove retweets because we need to analyze the tweets of the users, not the retweets. In this situation, retweets  provide no real value to our text exploration analysis as normally the users retweet to the airlines. Retweets are based off the tweets from the specified airlines' Twitter PR, and therefore, are likely going to be either of neutral or positive sentiment. Removing such retweets will hopefully remove any noise that will prevent the models from classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_to_replace= r'RT \\@.*'\n",
    "replace_value= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Awesome! '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "example1 = 'Awesome! RT @VirginAmerica: Watch nominated films at 35,000 feet. #MeetTheFleet #Oscars http://t.co/DnStITRzWy'\n",
    "example1 = re.sub(regex_to_replace, replace_value, example1)\n",
    "example1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove retweets from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                           @VirginAmerica What @dhepburn said.\n",
       "1                                                                                      @VirginAmerica plus you've added commercials to the experience... tacky.\n",
       "2                                                                                       @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
       "3                                    @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\n",
       "4                                                                                                       @VirginAmerica and it's a really big bad thing about it\n",
       "                                                                                  ...                                                                          \n",
       "14635                                                                                           @AmericanAir thank you we got on a different flight to Chicago.\n",
       "14636    @AmericanAir leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc\n",
       "14637                                                                                              @AmericanAir Please bring American Airlines to #BlackBerry10\n",
       "14638                   @AmericanAir you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\n",
       "14639                @AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?\n",
       "Name: text_cleaned, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text_cleaned'] = df_tweets['text_cleaned'].replace(to_replace=regex_to_replace, value=replace_value, regex=True)\n",
    "df_tweets['text_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove mentions\n",
    "Sometimes, users use mentions (for example, tweet mentions include @VirginAirlines, @JetBlue, etc., in other words, the airlines' handle). They normally appear in the beginning of the users' tweets. These mentions are not useful for sentiment analysis purposes because the vast majority of tweets have some sort of mention to one of the 6 airline companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_to_replace = r'\\@[\\w\\d]*'\n",
    "replace_value= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Thank you for the follow'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "example1 = '@VirginAmerica Thank you for the follow'\n",
    "example1 = re.sub(regex_to_replace, replace_value, example1)\n",
    "example1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove mentions from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                       What  said.\n",
       "1                                                                                         plus you've added commercials to the experience... tacky.\n",
       "2                                                                                          I didn't today... Must mean I need to take another trip!\n",
       "3                                       it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\n",
       "4                                                                                                          and it's a really big bad thing about it\n",
       "                                                                            ...                                                                    \n",
       "14635                                                                                            thank you we got on a different flight to Chicago.\n",
       "14636     leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc\n",
       "14637                                                                                               Please bring American Airlines to #BlackBerry10\n",
       "14638                    you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\n",
       "14639                 we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?\n",
       "Name: text_cleaned, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text_cleaned'] = df_tweets['text_cleaned'].replace(to_replace=regex_to_replace, value=replace_value, regex=True)\n",
    "df_tweets['text_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove HTTP links\n",
    " Users attach http links occasionally in their tweets. We need to remove HTTP links from the text, since they provide no real value to our sentiment analysis as well. From what we can see in the data, they are mostly links to articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_to_replace = r'https*://[^\\s]*'\n",
    "replace_value = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica when are you putting some great deals from PDX to LAS or from LAS to PDX show me your love! '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "example1 = '@VirginAmerica when are you putting some great deals from PDX to LAS or from LAS to PDX show me your love! http://t.co/enIQg0buzj'\n",
    "example1 = re.sub(regex_to_replace, replace_value, example1)\n",
    "example1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing HTTP Links from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                       What  said.\n",
       "1                                                                                         plus you've added commercials to the experience... tacky.\n",
       "2                                                                                          I didn't today... Must mean I need to take another trip!\n",
       "3                                       it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\n",
       "4                                                                                                          and it's a really big bad thing about it\n",
       "                                                                            ...                                                                    \n",
       "14635                                                                                            thank you we got on a different flight to Chicago.\n",
       "14636     leaving over 20 minutes Late Flight. No warnings or communication until we were 15 minutes Late Flight. That's called shitty customer svc\n",
       "14637                                                                                               Please bring American Airlines to #BlackBerry10\n",
       "14638                    you have my money, you change my flight, and don't answer your phones! Any other suggestions so I can make my commitment??\n",
       "14639                 we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?\n",
       "Name: text_cleaned, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text_cleaned'] = df_tweets['text_cleaned'].replace(to_replace=regex_to_replace, value=replace_value, regex=True)\n",
    "df_tweets['text_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>570076814585913344</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cancelled Flight</td>\n",
       "      <td>0.708</td>\n",
       "      <td>United</td>\n",
       "      <td>@united flt 1249 Cancelled Flightled and I get email @3:30 AM? What happened to courtesy phn call? Had to book diff airline &amp;amp; city</td>\n",
       "      <td>flt 1249 Cancelled Flightled and I get email :30 AM? What happened to courtesy phn call? Had to book diff airline &amp; city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "801  570076814585913344          negative                           1.0   \n",
       "\n",
       "       negativereason  negativereason_confidence airline  \\\n",
       "801  Cancelled Flight                      0.708  United   \n",
       "\n",
       "                                                                                                                                       text  \\\n",
       "801  @united flt 1249 Cancelled Flightled and I get email @3:30 AM? What happened to courtesy phn call? Had to book diff airline &amp; city   \n",
       "\n",
       "                                                                                                                  text_cleaned  \n",
       "801   flt 1249 Cancelled Flightled and I get email :30 AM? What happened to courtesy phn call? Had to book diff airline & city  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.loc[df_tweets['tweet_id'] == 570076814585913344]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.a. Remove Time\n",
    "We need to remove time from `df_tweets['text_cleaned']` because it will affect our emoticons extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_to_replace = r'[\\d]*\\:[\\d]{2}'\n",
    "replace_value = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@united flt 1249 Cancelled Flightled and I get email @ AM? What happened to courtesy phn call? Had to book diff airline'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "example1 = '@united flt 1249 Cancelled Flightled and I get email @3:30 AM? What happened to courtesy phn call? Had to book diff airline'\n",
    "example1 = re.sub(regex_to_replace, replace_value, example1)\n",
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_time_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>570076814585913344</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cancelled Flight</td>\n",
       "      <td>0.708</td>\n",
       "      <td>United</td>\n",
       "      <td>@united flt 1249 Cancelled Flightled and I get email @3:30 AM? What happened to courtesy phn call? Had to book diff airline &amp;amp; city</td>\n",
       "      <td>flt 1249 Cancelled Flightled and I get email :30 AM? What happened to courtesy phn call? Had to book diff airline &amp; city</td>\n",
       "      <td>flt 1249 Cancelled Flightled and I get email  AM? What happened to courtesy phn call? Had to book diff airline &amp; city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "801  570076814585913344          negative                           1.0   \n",
       "\n",
       "       negativereason  negativereason_confidence airline  \\\n",
       "801  Cancelled Flight                      0.708  United   \n",
       "\n",
       "                                                                                                                                       text  \\\n",
       "801  @united flt 1249 Cancelled Flightled and I get email @3:30 AM? What happened to courtesy phn call? Had to book diff airline &amp; city   \n",
       "\n",
       "                                                                                                                  text_cleaned  \\\n",
       "801   flt 1249 Cancelled Flightled and I get email :30 AM? What happened to courtesy phn call? Had to book diff airline & city   \n",
       "\n",
       "                                                                                                  text_cleaned_time_removed  \n",
       "801   flt 1249 Cancelled Flightled and I get email  AM? What happened to courtesy phn call? Had to book diff airline & city  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text_cleaned_time_removed'] = df_tweets['text_cleaned'].replace(to_replace=regex_to_replace, value=replace_value, regex=True)\n",
    "df_tweets.loc[df_tweets['tweet_id'] == 570076814585913344]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.b. Extract Emojis and Emoticons\n",
    "Rather than removing emojis and emoticons, emojis and emoticons can be seen as an integral part of the Internet language. Therefore, we should extract emojis and emoticons from the text if they exist, as they may be good features for sentiment analysis for \"Internet\"-speak.\n",
    "\n",
    "Emojis are special characters which are shown as actual visual images, whereas emoticons are keyboard characters arranged in a certain format so that it represents a human-like facial expression. Emoticons are not as commonly used compared to emojis anymore, but are still used occassionally by people, and it is in our best interests to also extract emoticons as well.\n",
    "\n",
    "We will use a third-party Python library called 'emot', which provides the ability to recognize and extract both emojis and emoticons. Github can be found here: https://github.com/NeelShah18/emot. There was an issue with the library where emot.emoticons would return different JSON structures, either `{'flag': False}` or `{'value': [], 'mean': [], 'location': [], 'flag': False}` whenever emoticons were not found. Upon manually investigating a few examples with such issues, we found out that although this was an issue, it was true that there were no emoticons in the texts of those examples. Therefore, we got around this issue by writing a variety of try/except catches. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the `emot` package manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': ['👨'], 'mean': [':man:'], 'location': [[14, 14]], 'flag': True}\n",
      "{'value': [':-)'], 'location': [[16, 19]], 'mean': ['Happy face smiley'], 'flag': True}\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "import emot\n",
    "text = \"I love python 👨 :-)\"\n",
    "print(emot.emoji(text))\n",
    "print(emot.emoticons(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hahaha 😂 YOU GUYS ARE AMAZING. I LOVE YOU GUYS!!!💗\n",
      "{'value': ['😂', '💗'], 'mean': [':face_with_tears_of_joy:', ':growing_heart:'], 'location': [[9, 9], [51, 51]], 'flag': True}\n",
      "{'value': [], 'location': [], 'mean': [], 'flag': False}\n"
     ]
    }
   ],
   "source": [
    "df_example1 = df_tweets.loc[df_tweets['tweet_id'] == 569198104806699008]\n",
    "print(df_example1['text_cleaned_time_removed'].to_string(index=False))\n",
    "print(emot.emoji(df_example1['text_cleaned_time_removed'].to_string(index=False)))\n",
    "print(emot.emoticons(df_example1['text_cleaned_time_removed'].to_string(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  we have a hot female pilot! Sweet! DCA to SFO! :-)\n",
      "{'value': [], 'mean': [], 'location': [], 'flag': False}\n",
      "{'value': [':-)'], 'location': [[49, 52]], 'mean': ['Happy face smiley'], 'flag': True}\n"
     ]
    }
   ],
   "source": [
    "df_example2 = df_tweets.loc[df_tweets['tweet_id'] == 568890074164809728]\n",
    "print(df_example2['text_cleaned_time_removed'].to_string(index=False))\n",
    "print(emot.emoji(df_example2['text_cleaned_time_removed'].to_string(index=False)))\n",
    "print(emot.emoticons(df_example2['text_cleaned_time_removed'].to_string(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracing Emojis and Emoticons from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will remove any emojis \n",
    "#accepts the following parameters: \n",
    "#(1) text_cleaned: roughly cleaned text in String to parse through and remove emojis\n",
    "#(2) emojis_flag: boolean created by emot.emoji(...), can be accessed by calling 'flag' key in dictionary (for more information see above)\n",
    "#(3) emojis: emojis object created by emot.emoji(...), can be accessed by calling 'value' key in dictionary (for more information see above)\n",
    "\n",
    "#returns the cleaned up text without emojis\n",
    "\n",
    "def remove_emojis(text_cleaned, emojis_flag, emojis):\n",
    "    text_cleaned_no_emojis = text_cleaned\n",
    "    \n",
    "    #print('text_cleaned @ remove_emojis: ' + text_cleaned)\n",
    "    #print('emojis_flag: ' + str(emojis_flag))\n",
    "    #print('emojis: ' + str(emojis))\n",
    "    \n",
    "    \n",
    "    #if flag is True, that means there are emojis, and we need to remove them\n",
    "    if emojis_flag:\n",
    "        for i in emojis:\n",
    "            \n",
    "            #rather than use location, we will match by String and see if we can remove it, because I'm lazy af lol\n",
    "            #print(str(i))\n",
    "            text_cleaned_no_emojis = text_cleaned_no_emojis.replace(i, '')\n",
    "            \n",
    "        if text_cleaned_no_emojis == text_cleaned:\n",
    "            print('Uh...Houston, we have a problem...for the following text: ' + text_cleaned + ', for row: ' + str(i))\n",
    "            print('The following emojis were not removed: ' + str(emojis))\n",
    "        \n",
    "    #print('resulting text_cleaned_no_emojis: ' + text_cleaned_no_emojis)\n",
    "    return text_cleaned_no_emojis  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will remove any emoticons \n",
    "#accepts the following parameters: \n",
    "#(1) text_cleaned: roughly cleaned text in String to parse through and remove emoticons\n",
    "#(2) emoticons_flag: boolean created by emot.emoticons(...), can be accessed by calling 'flag' key in dictionary (for more information see above)\n",
    "#(3) emoticons: emojis object created by emot.emoticons(...), can be accessed by calling 'value' key in dictionary (for more information see above)\n",
    "\n",
    "#returns the cleaned up text without emoticons\n",
    "\n",
    "def remove_emoticons(text_cleaned, emoticons_flag, emoticons):\n",
    "    text_cleaned_no_emoticons = text_cleaned\n",
    "    \n",
    "    #print('text_cleaned @ remove_emoticons: ' + text_cleaned)\n",
    "    #print('emoticons_flag: ' + str(emoticons_flag))\n",
    "    #print('emoticons: ' + str(emoticons))\n",
    "    \n",
    "    #if flag is True, that means there are emoticons, and we need to remove them\n",
    "    if emoticons_flag:\n",
    "        for i in emoticons:\n",
    "            \n",
    "            #rather than use location, we will match by String and see if we can remove it, because I'm lazy af lol\n",
    "            #print(str(i))\n",
    "            text_cleaned_no_emoticons = text_cleaned_no_emoticons.replace(i, '')\n",
    "            \n",
    "        if text_cleaned_no_emoticons == text_cleaned:\n",
    "            print('Uh...Houston, we have a problem...for the following text: ' + text_cleaned + ', for row: ' + str(i))\n",
    "            print('The following emojis were not removed: ' + str(emoticons))\n",
    "        \n",
    "    #print('resulting text_cleaned_no_emoticons: ' + text_cleaned_no_emoticons)\n",
    "    return text_cleaned_no_emoticons  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will extract any emojis into a separate 'emojis' column,\n",
    "#while also removing said emojis from column: 'text_cleaned' \n",
    "#to form a new column: 'text_cleaned_without_emojis_emoticons'\n",
    "\n",
    "#returns dataframe with the aforementioned new columns\n",
    "\n",
    "import emot\n",
    "\n",
    "#set logging so we don't output the try/except log messages unless if we want them\n",
    "#default level if WARNING\n",
    "import logging\n",
    "\n",
    "#in 'emojis_emoticons' column, it will hold the emot.emoji return dictionary (see above for examples)\n",
    "def extract_emojis(df_tweets, ):\n",
    "    df_tweets_2 = df_tweets\n",
    "    df_tweets_2['emojis_flag'] = ''\n",
    "    df_tweets_2['emojis'] = ''\n",
    "    df_tweets_2['emoticons_flag'] = ''\n",
    "    df_tweets_2['emoticons'] = ''\n",
    "        \n",
    "    #easier to just write out code to loop through dataframe\n",
    "    for i, row in df_tweets_2.iterrows():\n",
    "        #print(i)\n",
    "        #print(row)\n",
    "        text_cleaned_time_removed = df_tweets_2.at[i, 'text_cleaned_time_removed']\n",
    "        emojis = emot.emoji(text_cleaned_time_removed)\n",
    "        emoticons = emot.emoticons(text_cleaned_time_removed)\n",
    "        #print('EMOJIS: ' + str(emojis))\n",
    "        #print('EMOTICONS: ' + str(emoticons))\n",
    "        \n",
    "        ##When using emot.emoticons, the output when flag=False is inconsistent, it either is \n",
    "        #(a) {'value': [], 'mean': [], 'location': [], 'flag': False}, or \n",
    "        #(b) {'flag': False}\n",
    "        \n",
    "        #Therefore we have a bunch of try-except the aforementioned exception that will be raised, and set the values manually\n",
    "        try: \n",
    "            df_tweets_2.at[i, 'emojis_flag'] = emojis['flag']\n",
    "        except:\n",
    "            logging.debug('Unable to grab emojis flag at row number: ' + str(i))\n",
    "            df_tweets_2.at[i, 'emojis_flag'] = False\n",
    "            \n",
    "        try: \n",
    "             df_tweets_2.at[i, 'emojis'] = emojis['value']\n",
    "        except: \n",
    "            logging.debug('Unable to grab emojis value at row number: ' + str(i))\n",
    "            df_tweets_2.at[i, 'emojis'] = []\n",
    "            \n",
    "        try: \n",
    "            df_tweets_2.at[i, 'emoticons_flag'] = emoticons['flag']\n",
    "        except: \n",
    "            logging.debug('Unable to grab emoticons flag at row number: ' + str(i))\n",
    "            df_tweets_2.at[i, 'emoticons_flag'] = False\n",
    "            \n",
    "            \n",
    "        try: \n",
    "            df_tweets_2.at[i, 'emoticons'] = emoticons['value']\n",
    "        except: \n",
    "            logging.debug('Unable to grab emoticons value at row number: ' + str(i))\n",
    "            df_tweets_2.at[i, 'emoticons'] = []\n",
    "    \n",
    "        #afterwards, we need to remove emojis and emoticons from the\n",
    "        df_tweets_2.at[i, 'text_cleaned_without_emojis_emoticons'] = remove_emojis(\n",
    "            text_cleaned_time_removed,\n",
    "            df_tweets_2.at[i, 'emojis_flag'],\n",
    "            df_tweets_2.at[i, 'emojis']\n",
    "        )\n",
    "        \n",
    "        df_tweets_2.at[i, 'text_cleaned_without_emojis_emoticons'] = remove_emoticons(\n",
    "            df_tweets_2.at[i, 'text_cleaned_without_emojis_emoticons'],\n",
    "            df_tweets_2.at[i, 'emoticons_flag'],\n",
    "            df_tweets_2.at[i, 'emoticons']\n",
    "        )\n",
    "        \n",
    "    return df_tweets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweets['text_cleaned'] = df_tweets['text_cleaned'].apply(lambda text: BeautifulSoup(text, 'lxml').get_text())\n",
    "#df_tweets['text_cleaned']\n",
    "df_tweets = extract_emojis(df_tweets)\n",
    "#df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238    True\n",
      "Name: emojis_flag, dtype: object\n",
      "238    [😂, 💗]\n",
      "Name: emojis, dtype: object\n",
      "238    @VirginAmerica hahaha 😂@VirginAmerica YOU GUYS ARE AMAZING. I LOVE YOU GUYS!!!💗\n",
      "Name: text, dtype: object\n",
      "238     hahaha 😂 YOU GUYS ARE AMAZING. I LOVE YOU GUYS!!!💗\n",
      "Name: text_cleaned, dtype: object\n",
      "238     hahaha  YOU GUYS ARE AMAZING. I LOVE YOU GUYS!!!\n",
      "Name: text_cleaned_without_emojis_emoticons, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Sanity check: briefly check some examples where we know emojis and emoticons do exist\n",
    "df_example1 = df_tweets.loc[df_tweets['tweet_id'] == 569198104806699008]\n",
    "print(df_example1['emojis_flag'])\n",
    "print(df_example1['emojis'])\n",
    "print(df_example1['text'])\n",
    "print(df_example1['text_cleaned'])\n",
    "print(df_example1['text_cleaned_without_emojis_emoticons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18    True\n",
      "Name: emojis_flag, dtype: object\n",
      "18    [❤, ☺, 👍]\n",
      "Name: emojis, dtype: object\n",
      "18    I ❤️ flying @VirginAmerica. ☺️👍\n",
      "Name: text, dtype: object\n",
      "18    I ❤️ flying . ☺️👍\n",
      "Name: text_cleaned, dtype: object\n",
      "18    I ️ flying . ️\n",
      "Name: text_cleaned_without_emojis_emoticons, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Sanity check: briefly check some examples where we know emojis and emoticons do exist\n",
    "df_example1 = df_tweets.loc[df_tweets['tweet_id'] == 570270684619923457]\n",
    "print(df_example1['emojis_flag'])\n",
    "print(df_example1['emojis'])\n",
    "print(df_example1['text'])\n",
    "print(df_example1['text_cleaned'])\n",
    "print(df_example1['text_cleaned_without_emojis_emoticons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277    True\n",
      "Name: emoticons_flag, dtype: object\n",
      "277    [:-)]\n",
      "Name: emoticons, dtype: object\n",
      "277    @VirginAmerica we have a hot female pilot! Sweet! DCA to SFO! :-)\n",
      "Name: text, dtype: object\n",
      "277     we have a hot female pilot! Sweet! DCA to SFO! :-)\n",
      "Name: text_cleaned, dtype: object\n",
      "277     we have a hot female pilot! Sweet! DCA to SFO! \n",
      "Name: text_cleaned_without_emojis_emoticons, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Sanity check: briefly check some examples where we know emojis and emoticons do exist\n",
    "df_example2 = df_tweets.loc[df_tweets['tweet_id'] == 568890074164809728]\n",
    "print(df_example2['emoticons_flag'])\n",
    "print(df_example2['emoticons'])\n",
    "print(df_example2['text'])\n",
    "print(df_example2['text_cleaned'])\n",
    "print(df_example2['text_cleaned_without_emojis_emoticons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Extract and Remove Hashtags\n",
    "Some of the tweets have hashtag, which are keyword phrases spelled out with no spaces, and a pound sign at the front. They could offer more insight into the sentiment of the tweet. There were 3669 hashtags in the dataset in 2489 rows (some tweets evidently had more than one hashtag).\n",
    "\n",
    "For now, the easiest approach would be to remove the hashtags and the words inside during text cleaning. If we have time, we can try to analyze the best approaches to extract the words from the hashtags, and split them into meaningful words. This would be fairly challenging, however, as some tweets have multiple words that are not capitalized, so there is not an easy, fool-proof way of extracing hashtags into words. \n",
    "\n",
    "We also decided to extract the hashtags into a list inside a separate column for further data exploration and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hashtags contain relevant words, so let's try to make sure they are included in `text_cleaned_without_emojis_emoticons_hashtags`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489\n"
     ]
    }
   ],
   "source": [
    "print(len(df_tweets.loc[df_tweets['text'].str.contains('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts hashtags into a list, which is put into a new column called 'hashtags', whilst also removing hashtags from the text_cleaned\n",
    "def extract_and_remove_hashtags(df_tweets):\n",
    "    regex_to_replace= r'#(\\w+)'\n",
    "    replace_value= ''\n",
    "    df_tweets['hashtags'] = ''\n",
    "    df_tweets['text_cleaned_without_emojis_emoticons_hashtags'] = ''\n",
    "    \n",
    "    for i, row in df_tweets.iterrows():    \n",
    "        df_tweets.at[i, 'hashtags'] = re.findall(regex_to_replace, df_tweets.at[i, 'text_cleaned'].lower())\n",
    "        df_tweets.at[i, 'text_cleaned_without_emojis_emoticons_hashtags'] = re.sub(regex_to_replace, replace_value,  df_tweets.at[i, 'text_cleaned'])\n",
    "        \n",
    "        #create hashtags_flag\n",
    "        if df_tweets.at[i, 'hashtags'] == []:\n",
    "            df_tweets.at[i, 'hashtags_flag'] = False\n",
    "        else:\n",
    "            df_tweets.at[i, 'hashtags_flag'] = True\n",
    "        \n",
    "    return df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_time_removed</th>\n",
       "      <th>emojis_flag</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoticons_flag</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons_hashtags</th>\n",
       "      <th>hashtags_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to #BlackBerry10</td>\n",
       "      <td>Please bring American Airlines to #BlackBerry10</td>\n",
       "      <td>Please bring American Airlines to #BlackBerry10</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Please bring American Airlines to #BlackBerry10</td>\n",
       "      <td>[blackberry10]</td>\n",
       "      <td>Please bring American Airlines to</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "14637  569587242672398336           neutral                           1.0   \n",
       "\n",
       "      negativereason  negativereason_confidence   airline  \\\n",
       "14637            NaN                        NaN  American   \n",
       "\n",
       "                                                               text  \\\n",
       "14637  @AmericanAir Please bring American Airlines to #BlackBerry10   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "14637   Please bring American Airlines to #BlackBerry10   \n",
       "\n",
       "                              text_cleaned_time_removed emojis_flag emojis  \\\n",
       "14637   Please bring American Airlines to #BlackBerry10       False     []   \n",
       "\n",
       "      emoticons_flag emoticons  \\\n",
       "14637          False        []   \n",
       "\n",
       "                  text_cleaned_without_emojis_emoticons        hashtags  \\\n",
       "14637   Please bring American Airlines to #BlackBerry10  [blackberry10]   \n",
       "\n",
       "      text_cleaned_without_emojis_emoticons_hashtags hashtags_flag  \n",
       "14637             Please bring American Airlines to           True  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = extract_and_remove_hashtags(df_tweets)\n",
    "df_tweets.loc[df_tweets['tweet_id'] == 569587242672398336]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Convert text to Lowercase\n",
    "We need to convert the text to lower case. This is done so that when we tokenize the words, there won't be words that are grouped separately just because of case sensitivity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_time_removed</th>\n",
       "      <th>emojis_flag</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoticons_flag</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons_hashtags</th>\n",
       "      <th>hashtags_flag</th>\n",
       "      <th>text_cleaned_lower_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>[]</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>False</td>\n",
       "      <td>what  said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[]</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>False</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[]</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>False</td>\n",
       "      <td>i didn't today... must mean i need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>[]</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>False</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>[]</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>False</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "                                                                                                   text_cleaned  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "                                                                                      text_cleaned_time_removed  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "  emojis_flag emojis emoticons_flag emoticons  \\\n",
       "0       False     []          False        []   \n",
       "1       False     []          False        []   \n",
       "2       False     []          False        []   \n",
       "3       False     []          False        []   \n",
       "4       False     []          False        []   \n",
       "\n",
       "                                                                          text_cleaned_without_emojis_emoticons  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "  hashtags  \\\n",
       "0       []   \n",
       "1       []   \n",
       "2       []   \n",
       "3       []   \n",
       "4       []   \n",
       "\n",
       "                                                                 text_cleaned_without_emojis_emoticons_hashtags  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "  hashtags_flag  \\\n",
       "0         False   \n",
       "1         False   \n",
       "2         False   \n",
       "3         False   \n",
       "4         False   \n",
       "\n",
       "                                                                                        text_cleaned_lower_case  \n",
       "0                                                                                                   what  said.  \n",
       "1                                                     plus you've added commercials to the experience... tacky.  \n",
       "2                                                      i didn't today... must mean i need to take another trip!  \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse  \n",
       "4                                                                      and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['text_cleaned_lower_case'] = \\\n",
    "    df_tweets['text_cleaned_without_emojis_emoticons_hashtags'].apply(lambda text: text.lower())\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Internet Slang abbreviations\n",
    "We need to convert the Internet slang abbreviations into readable English lexicon. To do so, we will use a dictionary of commonly used slang words, which we have used partly from here: \n",
    "https://github.com/Deffro/text-preprocessing-techniques/blob/master/slang.txt. We modified the dictionary to be in CSV format, as well as changing some of the key-value pairs as they were improperly written. Each Internet slang abbreviation key is linked to its respective value, which would be the complete form of the abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slang_abbreviation</th>\n",
       "      <th>complete_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2day</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nite</td>\n",
       "      <td>tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4u</td>\n",
       "      <td>for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4ward</td>\n",
       "      <td>forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3</td>\n",
       "      <td>anyplace, anywhere, anytime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>yuge</td>\n",
       "      <td>huge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>yw</td>\n",
       "      <td>you are welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>ywa</td>\n",
       "      <td>you are welcome anyway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>zomg</td>\n",
       "      <td>oh my god!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>zzz</td>\n",
       "      <td>sleeping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    slang_abbreviation                complete_form\n",
       "0                 2day                        today\n",
       "1                2nite                      tonight\n",
       "2                   4u                      for you\n",
       "3                4ward                      forward\n",
       "4                   a3  anyplace, anywhere, anytime\n",
       "..                 ...                          ...\n",
       "285               yuge                         huge\n",
       "286                 yw              you are welcome\n",
       "287                ywa       you are welcome anyway\n",
       "288               zomg                   oh my god!\n",
       "289                zzz                     sleeping\n",
       "\n",
       "[290 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slang = pd.read_csv('..\\data\\slang.csv')\n",
    "df_slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviation(text, abbreviation, complete_form):\n",
    "    return re.sub(abbreviation, complete_form, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o we fly straight into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you please sort this out soon!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity check, testing above function\n",
    "regex_expression = re.compile('\\\\b'+'plz'+'\\\\b')\n",
    "regex_to_replace =  'please'\n",
    "\n",
    "replace_abbreviation(\n",
    "    'o we fly straight into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you plz sort this out soon!',\n",
    "    regex_expression,\n",
    "    regex_to_replace\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o we fly straight into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you plz sort this out soon!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another sanity check, but a negative case this time around\n",
    "#the word 'straight' should remain the same, even though the dictionary contains the abbreviation 'aight'\n",
    "regex_expression = re.compile('\\\\b'+'aight'+'\\\\b')\n",
    "regex_to_replace = 'alright'\n",
    "\n",
    "replace_abbreviation(\n",
    "    'o we fly straight into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you plz sort this out soon!',\n",
    "    regex_expression,\n",
    "    regex_to_replace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will find slang abbreviations and replace them with the complete forms\n",
    "#needs df_tweets and df_slang\n",
    "#returns a new dataframe with new column 'text_cleaned_without_emojis_emoticons_hashtags_abbreviations'\n",
    "\n",
    "#we will iterate over every row/tweet and and see if there are instances of slang abbreviations\n",
    "#computationally expensive, but because there are only 14000 rows, it's not too bad\n",
    "#there is no easy way of detecting whether a word is an abbreviation or not using Spacy, NLTK, or spellchecker\n",
    "def find_slang_abbreviations_and_replace_with_complete_form(df_tweets, df_slang):\n",
    "    df_tweets['text_cleaned_no_abbreviations'] = ''\n",
    "    \n",
    "    for i, tweet_row in df_tweets.iterrows():\n",
    "        df_tweets.at[i, 'text_cleaned_no_abbreviations'] = \\\n",
    "            df_tweets.at[i, 'text_cleaned_lower_case']\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('at row number: ' + str(i))\n",
    "        \n",
    "        for j, slang_row in df_slang.iterrows():\n",
    "            #print(slang_row)\n",
    "            slang_abbreviation = df_slang.at[j, 'slang_abbreviation'] \n",
    "            complete_form = df_slang.at[j, 'complete_form']\n",
    "            \n",
    "            #print(\"slang_abbreviation: \" + slang_abbreviation)\n",
    "            #print(\"complete_form: \" + complete_form)\n",
    "            \n",
    "            regex_expression = re.compile('\\\\b'+slang_abbreviation+'\\\\b')\n",
    "            regex_to_replace = complete_form\n",
    "            \n",
    "            df_tweets.at[i, 'text_cleaned_no_abbreviations'] = \\\n",
    "                replace_abbreviation(\n",
    "                    df_tweets.at[i, 'text_cleaned_no_abbreviations'], \n",
    "                    regex_expression,\n",
    "                    regex_to_replace\n",
    "            )\n",
    "            #print(\"current: \" + df_tweets.at[i, 'text_cleaned_no_abbreviations'])\n",
    "\n",
    "        \n",
    "    return df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at row number: 0\n",
      "at row number: 1000\n",
      "at row number: 2000\n",
      "at row number: 3000\n",
      "at row number: 4000\n",
      "at row number: 5000\n",
      "at row number: 6000\n",
      "at row number: 7000\n",
      "at row number: 8000\n",
      "at row number: 9000\n",
      "at row number: 10000\n",
      "at row number: 11000\n",
      "at row number: 12000\n",
      "at row number: 13000\n",
      "at row number: 14000\n"
     ]
    }
   ],
   "source": [
    "#create new column 'text_cleaned_no_abbreviations' in df_tweets\n",
    "#by calling find_slang_abbreviations_and_replace_with_complete_form function\n",
    "df_tweets = find_slang_abbreviations_and_replace_with_complete_form(df_tweets, df_slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_time_removed</th>\n",
       "      <th>emojis_flag</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoticons_flag</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons_hashtags</th>\n",
       "      <th>hashtags_flag</th>\n",
       "      <th>text_cleaned_lower_case</th>\n",
       "      <th>text_cleaned_no_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>[]</td>\n",
       "      <td>What  said.</td>\n",
       "      <td>False</td>\n",
       "      <td>what  said.</td>\n",
       "      <td>what  said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[]</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>False</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[]</td>\n",
       "      <td>I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>False</td>\n",
       "      <td>i didn't today... must mean i need to take another trip!</td>\n",
       "      <td>i didn't today... must mean i need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>[]</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>False</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>[]</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>False</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "                                                                                                   text_cleaned  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "                                                                                      text_cleaned_time_removed  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "  emojis_flag emojis emoticons_flag emoticons  \\\n",
       "0       False     []          False        []   \n",
       "1       False     []          False        []   \n",
       "2       False     []          False        []   \n",
       "3       False     []          False        []   \n",
       "4       False     []          False        []   \n",
       "\n",
       "                                                                          text_cleaned_without_emojis_emoticons  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "  hashtags  \\\n",
       "0       []   \n",
       "1       []   \n",
       "2       []   \n",
       "3       []   \n",
       "4       []   \n",
       "\n",
       "                                                                 text_cleaned_without_emojis_emoticons_hashtags  \\\n",
       "0                                                                                                   What  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      I didn't today... Must mean I need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "  hashtags_flag  \\\n",
       "0         False   \n",
       "1         False   \n",
       "2         False   \n",
       "3         False   \n",
       "4         False   \n",
       "\n",
       "                                                                                        text_cleaned_lower_case  \\\n",
       "0                                                                                                   what  said.   \n",
       "1                                                     plus you've added commercials to the experience... tacky.   \n",
       "2                                                      i didn't today... must mean i need to take another trip!   \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse   \n",
       "4                                                                      and it's a really big bad thing about it   \n",
       "\n",
       "                                                                                  text_cleaned_no_abbreviations  \n",
       "0                                                                                                   what  said.  \n",
       "1                                                     plus you've added commercials to the experience... tacky.  \n",
       "2                                                      i didn't today... must mean i need to take another trip!  \n",
       "3   it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse  \n",
       "4                                                                      and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_time_removed</th>\n",
       "      <th>emojis_flag</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoticons_flag</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons_hashtags</th>\n",
       "      <th>hashtags_flag</th>\n",
       "      <th>text_cleaned_lower_case</th>\n",
       "      <th>text_cleaned_no_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2788</th>\n",
       "      <td>568899587424931840</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1.0</td>\n",
       "      <td>United</td>\n",
       "      <td>@united so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!</td>\n",
       "      <td>so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!</td>\n",
       "      <td>so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!</td>\n",
       "      <td>[]</td>\n",
       "      <td>so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!</td>\n",
       "      <td>False</td>\n",
       "      <td>so we fly into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you plz sort this out soon!</td>\n",
       "      <td>so we fly into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you please sort this out soon!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "2788  568899587424931840          negative                           1.0   \n",
       "\n",
       "     negativereason  negativereason_confidence airline  \\\n",
       "2788    Late Flight                        1.0  United   \n",
       "\n",
       "                                                                                                                                             text  \\\n",
       "2788  @united so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!   \n",
       "\n",
       "                                                                                                                              text_cleaned  \\\n",
       "2788   so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!   \n",
       "\n",
       "                                                                                                                 text_cleaned_time_removed  \\\n",
       "2788   so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!   \n",
       "\n",
       "     emojis_flag emojis emoticons_flag emoticons  \\\n",
       "2788       False     []          False        []   \n",
       "\n",
       "                                                                                                     text_cleaned_without_emojis_emoticons  \\\n",
       "2788   so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!   \n",
       "\n",
       "     hashtags  \\\n",
       "2788       []   \n",
       "\n",
       "                                                                                            text_cleaned_without_emojis_emoticons_hashtags  \\\n",
       "2788   so we fly into SFO and Honululu gets pushed back 3.5 hours and now it looks like more delays.  I beg of you plz sort this out soon!   \n",
       "\n",
       "     hashtags_flag  \\\n",
       "2788         False   \n",
       "\n",
       "                                                                                                                   text_cleaned_lower_case  \\\n",
       "2788   so we fly into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you plz sort this out soon!   \n",
       "\n",
       "                                                                                                                text_cleaned_no_abbreviations  \n",
       "2788   so we fly into sfo and honululu gets pushed back 3.5 hours and now it looks like more delays.  i beg of you please sort this out soon!  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "df_tweets.loc[df_tweets['tweet_id'] == 568899587424931840]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Removing StopWords and Punctuation and numbers, Lemmatization, and Tokenization\n",
    "We want to remove stop words because most common words such as \"the\", \"and\", \"I\", \"you\"  are not likely to add any value to our sentiment analysis. Removing stopwords will therefore reduce the noise that will reduce the effectiveness of our eventual sentiment analysis model. Removing stopwords is also beneficial in the sense that it also removes common contractions such as \"I've, you're, etc.\"\n",
    "\n",
    "We also want to remove punctuation because punctuation is not useful to our sentiment analysis. Punctutation might help determine the intensity, or polarity of text, but that is not relevant to sentiment. Similar to punctuation, we also want to remove numbers.\n",
    "\n",
    "Lastly, we will need to perform lemmatization, so that we can reduce the  words to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacy model\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at row number: 0\n",
      "at row number: 1000\n",
      "at row number: 2000\n",
      "at row number: 3000\n",
      "at row number: 4000\n",
      "at row number: 5000\n",
      "at row number: 6000\n",
      "at row number: 7000\n",
      "at row number: 8000\n",
      "at row number: 9000\n",
      "at row number: 10000\n",
      "at row number: 11000\n",
      "at row number: 12000\n",
      "at row number: 13000\n",
      "at row number: 14000\n"
     ]
    }
   ],
   "source": [
    "df_tweets['text_list_no_stop_words'] = ''\n",
    "df_tweets['lemmas_list'] = ''\n",
    "    \n",
    "for i, row in df_tweets.iterrows():   \n",
    "    if i % 1000 == 0:\n",
    "        print('at row number: ' + str(i))\n",
    "    \n",
    "    text = df_tweets.at[i, 'text_cleaned_no_abbreviations']\n",
    "    \n",
    "    #tokenize text into list of tokens\n",
    "    token_list = nlp(text)\n",
    "    \n",
    "    text_list_no_stop_words = []\n",
    "    lemmas_list = []\n",
    "    \n",
    "    #remove stop words, and lemmatize\n",
    "    for token in token_list:\n",
    "        #print(str(token.is_stop))\n",
    "        #print(str(token.pos_))\n",
    "        \n",
    "        #if token is not a stop word, and not punctuation, and not a number, \n",
    "        #then it is useful to us and we store them in our lists\n",
    "        #token.is_digit doesn't really work all that well if periods are involved (e.g. \"5.5\")\n",
    "        if (token.is_stop == False) & (not token.is_punct) & (not token.is_space) & (not token.like_num):\n",
    "            text_list_no_stop_words.append(token.text)\n",
    "            lemmas_list.append(token.lemma_)\n",
    "    \n",
    "    #print('text_list_no_stop_words:' + str(text_list_no_stop_words))\n",
    "    #print('lemmas_list:' + str(lemmas_list))\n",
    "    df_tweets.at[i, 'text_list_no_stop_words'] = \" \".join(text_list_no_stop_words)\n",
    "    df_tweets.at[i, 'lemmas_list'] = \" \".join(lemmas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Manually Remove Emoticons that are not actually Emoticons\n",
    "Some \"emoticons\" that were extracted, such as `d:, DX, D8, XP, D:, :c,` are not actually emoticons. They were part of words. The easiest way is to manually remove them now rather than rewrite the emoticon extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tweets.loc[df_tweets['emoticons_flag'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a given emoticon, we remove it from the dataframe of tweets\n",
    "#remove it from `emoticons` column\n",
    "#if it's empty then we set the emoticon_flag to be false\n",
    "def remove_emoticon(df, emoticon_string):\n",
    "    #loop through df to \n",
    "    for i, row in df.iterrows():    \n",
    "        if df.at[i, 'emoticons_flag'] == True:\n",
    "            #tweet_emoticons_list = df.at[i, 'emoticons']\n",
    "            #tweet_emoticons_list = list(tweet_emoticons.split(\",\"))\n",
    "            \n",
    "            #print(\"df.at[i, 'emoticons']: \" + str(df.at[i, 'tweet_id']))\n",
    "            for tweet_emoticon in df.at[i, 'emoticons']:\n",
    "                #print('tweet_emoticon: ' + tweet_emoticon)\n",
    "                #strip brackets, quote, and spaces\n",
    "                #tweet_emoticon = tweet_emoticon.strip('[]')\n",
    "                #tweet_emoticon = tweet_emoticon.replace(\"\\'\", \"\")\n",
    "                #tweet_emoticon = tweet_emoticon.strip()\n",
    "                \n",
    "                if emoticon_string == tweet_emoticon:\n",
    "                    #print(\"emoticon_string: \" + emoticon_string + \" matches emoticon_string: \" + tweet_emoticon)\n",
    "                    df.at[i, 'emoticons'].remove(tweet_emoticon)\n",
    "                        \n",
    "            #if all emoticons have been removed, then we need to set emoticon_flag to false\n",
    "            if df.at[i, 'emoticons'] == []:\n",
    "                #print('set flag to false')\n",
    "                df.at[i, 'emoticons_flag'] = False\n",
    "                \n",
    "            #after removing emoticons \n",
    "            #df.at[i, 'emoticons'] = \" \".join(tweet_emoticons_list)\n",
    "            #print(\"df.at[i, 'emoticons']: \" + df.at[i, 'emoticons'])\n",
    "    \n",
    "    return df           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = remove_emoticon(df_tweets, 'd:')\n",
    "df_tweets = remove_emoticon(df_tweets, 'DX')\n",
    "df_tweets = remove_emoticon(df_tweets, 'D8')\n",
    "df_tweets = remove_emoticon(df_tweets, 'XP')\n",
    "df_tweets = remove_emoticon(df_tweets, 'D:')\n",
    "df_tweets = remove_emoticon(df_tweets, ':c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_time_removed</th>\n",
       "      <th>emojis_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>emoticons_flag</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_cleaned_without_emojis_emoticons_hashtags</th>\n",
       "      <th>hashtags_flag</th>\n",
       "      <th>text_cleaned_lower_case</th>\n",
       "      <th>text_cleaned_no_abbreviations</th>\n",
       "      <th>text_list_no_stop_words</th>\n",
       "      <th>lemmas_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12405</th>\n",
       "      <td>570199267668676609</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1.0</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401</td>\n",
       "      <td>Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401</td>\n",
       "      <td>Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>[d:]</td>\n",
       "      <td>Delaye AA3186 - Missed due to delay: AA3186 - New flight now delaye AA2401</td>\n",
       "      <td>[]</td>\n",
       "      <td>Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401</td>\n",
       "      <td>False</td>\n",
       "      <td>delayed: aa3186 - missed due to delay: aa3186 - new flight now delayed: aa2401</td>\n",
       "      <td>delayed: aa3186 - missed due to delay: aa3186 - new flight now delayed: aa2401</td>\n",
       "      <td>delayed aa3186 missed delay aa3186 new flight delayed aa2401</td>\n",
       "      <td>delay aa3186 miss delay aa3186 new flight delay aa2401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "12405  570199267668676609          negative                           1.0   \n",
       "\n",
       "      negativereason  negativereason_confidence   airline  \\\n",
       "12405    Late Flight                        1.0  American   \n",
       "\n",
       "                                                                                              text  \\\n",
       "12405  @AmericanAir Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401   \n",
       "\n",
       "                                                                          text_cleaned  \\\n",
       "12405   Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401   \n",
       "\n",
       "                                                             text_cleaned_time_removed  \\\n",
       "12405   Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401   \n",
       "\n",
       "      emojis_flag  ... emoticons_flag emoticons  \\\n",
       "12405       False  ...           True      [d:]   \n",
       "\n",
       "                                             text_cleaned_without_emojis_emoticons  \\\n",
       "12405   Delaye AA3186 - Missed due to delay: AA3186 - New flight now delaye AA2401   \n",
       "\n",
       "      hashtags  \\\n",
       "12405       []   \n",
       "\n",
       "                                        text_cleaned_without_emojis_emoticons_hashtags  \\\n",
       "12405   Delayed: AA3186 - Missed due to delay: AA3186 - New flight now delayed: AA2401   \n",
       "\n",
       "      hashtags_flag  \\\n",
       "12405         False   \n",
       "\n",
       "                                                               text_cleaned_lower_case  \\\n",
       "12405   delayed: aa3186 - missed due to delay: aa3186 - new flight now delayed: aa2401   \n",
       "\n",
       "                                                         text_cleaned_no_abbreviations  \\\n",
       "12405   delayed: aa3186 - missed due to delay: aa3186 - new flight now delayed: aa2401   \n",
       "\n",
       "                                            text_list_no_stop_words  \\\n",
       "12405  delayed aa3186 missed delay aa3186 new flight delayed aa2401   \n",
       "\n",
       "                                                  lemmas_list  \n",
       "12405  delay aa3186 miss delay aa3186 new flight delay aa2401  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "df_tweets.loc[df_tweets['tweet_id'] == 570199267668676609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tweets.loc[df_tweets['emoticons_flag'] == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data is now complete, we'll save the dataframe to a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('..\\data\\Tweets_cleaned.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
